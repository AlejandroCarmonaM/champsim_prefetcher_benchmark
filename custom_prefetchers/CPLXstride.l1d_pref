#include "cache.h"
#include <cstdint>

#define STRIDE_TABLE_SIZE 2048
#define CSPT_SIZE 2048 // size for the complex stride prediction table

// Each history table entry has: PC, last cache line address, and the last three observed strides
struct StrideEntry {
    uint64_t ip;
    uint64_t last_cl_addr;
    int64_t stride1; 
    int64_t stride2; 
    int64_t stride3; 
};

static StrideEntry stride_table[STRIDE_TABLE_SIZE];

// Complex Stride Prediction Table (CSPT) holds the "predicted stride" based on the last three strides and IP
struct CSPTEntry {
    int64_t predicted_stride;
};

static CSPTEntry cspt_table[CSPT_SIZE];

// It shifts and XORs the values to generate a fairly distributed index for CSPT lookups
static inline uint64_t compute_cspt_index(uint64_t ip, int64_t s1, int64_t s2, int64_t s3)
{
    // mask to 16 bits each for strides
    uint64_t us1 = (uint64_t)(s1 & 0xFFFF);
    uint64_t us2 = (uint64_t)(s2 & 0xFFFF);
    uint64_t us3 = (uint64_t)(s3 & 0xFFFF);
    // combine IP and strides
    uint64_t hash_val = (ip << 32) ^ (us1 << 16) ^ (us2 << 8) ^ us3;
    return hash_val % CSPT_SIZE;
}

// It shifts and XORs the values to generate a fairly distributed index for CSPT lookups (Avalanche)
static inline uint64_t compute_cspt_index_avalanche(uint64_t ip, int64_t s1, int64_t s2, int64_t s3)
{
    // Mask to 16 bits
    uint64_t us1 = (uint64_t)(s1 & 0xFFFF);
    uint64_t us2 = (uint64_t)(s2 & 0xFFFF);
    uint64_t us3 = (uint64_t)(s3 & 0xFFFF);

    // Combine IP and strides
    uint64_t val = (ip << 32) ^ (us1 << 16) ^ (us2 << 8) ^ us3;

    // Simple avalanche mix
    val ^= val >> 33;
    val *= 0xb492b66fbe98f273ULL;
    val ^= val >> 33;
    val *= 0x9ae16a3b2f90404fULL;
    val ^= val >> 33;

    return val % CSPT_SIZE;
}

void CACHE::l1d_prefetcher_initialize()
{
    // Initialize history table
    for(int i = 0; i < STRIDE_TABLE_SIZE; i++){
        stride_table[i].ip = 0;
        stride_table[i].last_cl_addr = 0;
        stride_table[i].stride1 = 0;
        stride_table[i].stride2 = 0;
        stride_table[i].stride3 = 0;
    }
    // Initialize CSPT
    for(int i = 0; i < CSPT_SIZE; i++){
        cspt_table[i].predicted_stride = 0;
    }
}

void CACHE::l1d_prefetcher_operate(uint64_t addr, uint64_t ip, uint8_t cache_hit, uint8_t type)
{
    uint64_t cl_addr = addr >> LOG2_BLOCK_SIZE;  // cache line address calculated from addr
    int index = -1;

    // Look up or create entry in history table
    for(int i = 0; i < STRIDE_TABLE_SIZE; i++){
        if(stride_table[i].ip == ip){
            index = i;
            break;
        }
    }
    // If not found, allocate a new entry
    if(index == -1) {
        for(int i = 0; i < STRIDE_TABLE_SIZE; i++){
            // first empty slot is used
            if(stride_table[i].ip == 0){
                index = i;
                stride_table[i].ip = ip; // store IP
                stride_table[i].last_cl_addr = cl_addr; // store last cache line address
                stride_table[i].stride1 = 0; // initialize strides
                stride_table[i].stride2 = 0; 
                stride_table[i].stride3 = 0; 
                break; // exit loop after allocation
            }
        }
        // either we allocated a new entry or table is full, in both cases return
        return;
    }

    // Calculate new stride
    int64_t new_stride = (int64_t)cl_addr - (int64_t)stride_table[index].last_cl_addr;
    if(new_stride == 0) {
        // same cache block, update and return (no prefetch)
        stride_table[index].last_cl_addr = cl_addr;
        return;
    }

    // 1) TRAINNING PHASE: train CSPT with old pattern
    uint64_t old_index = compute_cspt_index_avalanche(ip, stride_table[index].stride1, 
                                            stride_table[index].stride2, stride_table[index].stride3);
    int64_t old_predicted = cspt_table[old_index].predicted_stride;
    // update CSPT if old stride is not the same as new stride
    if(old_predicted != new_stride) {
        // update predicted stride for this pattern
        cspt_table[old_index].predicted_stride = new_stride;
    }

    // 2) Shift old strides; store new_stride as stride1
    stride_table[index].stride3 = stride_table[index].stride2;
    stride_table[index].stride2 = stride_table[index].stride1;
    stride_table[index].stride1 = new_stride;
    stride_table[index].last_cl_addr = cl_addr;

    // 3) INFERENCE: Now compute new CSPT index and get predicted stride
    uint64_t new_index_cspt = compute_cspt_index_avalanche(ip, stride_table[index].stride1,
                                                stride_table[index].stride2, stride_table[index].stride3);
    int64_t predicted_stride = cspt_table[new_index_cspt].predicted_stride;
    if(predicted_stride != 0) {
        // issue prefetch
        uint64_t next_addr = addr + (predicted_stride << LOG2_BLOCK_SIZE);
        if((next_addr >> LOG2_PAGE_SIZE) == (addr >> LOG2_PAGE_SIZE)){
            prefetch_line(ip, addr, next_addr, FILL_L1, 0);
        }
    }
}

void CACHE::l1d_prefetcher_cache_fill(uint64_t addr, uint32_t set, uint32_t way, 
                                      uint8_t prefetch, uint64_t evicted_addr, uint32_t metadata_in)
{
    // not used
}

void CACHE::l1d_prefetcher_final_stats()
{
    // not used
}